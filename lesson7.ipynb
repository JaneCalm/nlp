{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "hw_lesson_7_transf_attention.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNlsoTuhbQUs"
      },
      "source": [
        "Запустить seq2seq, seq2seq с внимаием для перевода русских слов + описать наблюдения по качеству\n",
        "Данные в папке data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUhYXWscbQUt"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvGZ5vsQbQUy"
      },
      "source": [
        "batch_size = 1024\n",
        "epochs = 20\n",
        "latent_dim = 256\n",
        "num_samples = 20000\n",
        "data_path = '/content/drive/My Drive/data/rus-eng/rus.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiLt6gNAbQU1",
        "outputId": "cbb260bc-fcd0-431e-f2ea-a71a1c0b008a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read()[:1000]\n",
        "    print(lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go.\tМарш!\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1159202 (shanghainese)\n",
            "Go.\tИди.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #5898247 (marafon)\n",
            "Go.\tИдите.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #5898250 (marafon)\n",
            "Hi.\tЗдравствуйте.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #402127 (odexed)\n",
            "Hi.\tПривет!\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #466968 (katjka)\n",
            "Hi.\tХай.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #467233 (timsa)\n",
            "Hi.\tЗдрасте.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #3803577 (marafon)\n",
            "Hi.\tЗдоро́во!\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #3854188 (marafon)\n",
            "Run!\tБеги!\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #1569978 (Biga)\n",
            "Run!\tБегите!\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #2770234 (marafon)\n",
            "Run.\tБеги!\tCC-BY 2.0 (France) Attribution: tatoeba.org #4008918 (JSakuragi) & #1569978 (Bi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3HmccMlbQU5"
      },
      "source": [
        "### Предобработка текста в последовательности"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOLV3jvhbQU6"
      },
      "source": [
        "# Собираем из текстов токены и делаем one-hot вектора на каждый токен\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split('\\t')\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYPZJVkdbQU9"
      },
      "source": [
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaH9aoIfbQVG"
      },
      "source": [
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Wee66B2bQVT"
      },
      "source": [
        "# Задаем размеры последовательности\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
        "    decoder_target_data[i, t:, target_token_index[' ']] = 1."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW5q4tVybQVZ"
      },
      "source": [
        "### Обучение базового seq2seq\n",
        "\n",
        "Генерация текста. В данной модели в LSTM-ячейку подается посимвольная последовательность"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YixJ7DJZbQVa",
        "outputId": "264cfc8c-a0c2-42ee-d3df-0a6042d95078",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "\n",
        "# Подача выхода энкодера в декодер\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Подача выхода декодера в модель\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)\n",
        "\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "16/16 [==============================] - 63s 4s/step - loss: 2.6501 - accuracy: 0.6679 - val_loss: 1.5206 - val_accuracy: 0.7391\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 62s 4s/step - loss: 1.2628 - accuracy: 0.7648 - val_loss: 1.3619 - val_accuracy: 0.7392\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 63s 4s/step - loss: 1.1689 - accuracy: 0.7593 - val_loss: 1.2444 - val_accuracy: 0.7354\n",
            "Epoch 4/20\n",
            "16/16 [==============================] - 67s 4s/step - loss: 1.0548 - accuracy: 0.7626 - val_loss: 1.1611 - val_accuracy: 0.7366\n",
            "Epoch 5/20\n",
            "16/16 [==============================] - 63s 4s/step - loss: 0.9991 - accuracy: 0.7603 - val_loss: 1.0968 - val_accuracy: 0.7330\n",
            "Epoch 6/20\n",
            "16/16 [==============================] - 63s 4s/step - loss: 0.9432 - accuracy: 0.7592 - val_loss: 1.0221 - val_accuracy: 0.7357\n",
            "Epoch 7/20\n",
            "16/16 [==============================] - 62s 4s/step - loss: 0.8967 - accuracy: 0.7703 - val_loss: 0.9859 - val_accuracy: 0.7389\n",
            "Epoch 8/20\n",
            "16/16 [==============================] - 62s 4s/step - loss: 0.8623 - accuracy: 0.7699 - val_loss: 0.9520 - val_accuracy: 0.7342\n",
            "Epoch 9/20\n",
            "16/16 [==============================] - 62s 4s/step - loss: 0.8331 - accuracy: 0.7706 - val_loss: 0.9301 - val_accuracy: 0.7462\n",
            "Epoch 10/20\n",
            "16/16 [==============================] - 62s 4s/step - loss: 0.8174 - accuracy: 0.7775 - val_loss: 0.9158 - val_accuracy: 0.7409\n",
            "Epoch 11/20\n",
            "16/16 [==============================] - 62s 4s/step - loss: 0.7956 - accuracy: 0.7815 - val_loss: 0.8918 - val_accuracy: 0.7521\n",
            "Epoch 12/20\n",
            "16/16 [==============================] - 62s 4s/step - loss: 0.7777 - accuracy: 0.7909 - val_loss: 0.8745 - val_accuracy: 0.7645\n",
            "Epoch 13/20\n",
            "16/16 [==============================] - 62s 4s/step - loss: 0.7617 - accuracy: 0.7971 - val_loss: 0.8565 - val_accuracy: 0.7705\n",
            "Epoch 14/20\n",
            "16/16 [==============================] - 62s 4s/step - loss: 0.7456 - accuracy: 0.8028 - val_loss: 0.8395 - val_accuracy: 0.7782\n",
            "Epoch 15/20\n",
            "16/16 [==============================] - 62s 4s/step - loss: 0.7289 - accuracy: 0.8092 - val_loss: 0.8202 - val_accuracy: 0.7857\n",
            "Epoch 16/20\n",
            "16/16 [==============================] - 62s 4s/step - loss: 0.7123 - accuracy: 0.8140 - val_loss: 0.8018 - val_accuracy: 0.7908\n",
            "Epoch 17/20\n",
            "16/16 [==============================] - 62s 4s/step - loss: 0.6952 - accuracy: 0.8175 - val_loss: 0.7833 - val_accuracy: 0.7926\n",
            "Epoch 18/20\n",
            "16/16 [==============================] - 62s 4s/step - loss: 0.6796 - accuracy: 0.8212 - val_loss: 0.7674 - val_accuracy: 0.7985\n",
            "Epoch 19/20\n",
            "16/16 [==============================] - 62s 4s/step - loss: 0.6640 - accuracy: 0.8250 - val_loss: 0.7497 - val_accuracy: 0.8011\n",
            "Epoch 20/20\n",
            "16/16 [==============================] - 61s 4s/step - loss: 0.6497 - accuracy: 0.8273 - val_loss: 0.7344 - val_accuracy: 0.8024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzV5RMEObQVf"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwSScQfmbQVk",
        "outputId": "1d2ebbc3-1d44-4533-8257-92f5d29542a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for seq_index in range(0, 500, 50):\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: То                                                           \n",
            "-\n",
            "Input sentence: I see.\n",
            "Decoded sentence: Я не                                                         \n",
            "-\n",
            "Input sentence: He ran.\n",
            "Decoded sentence: Он                                                           \n",
            "-\n",
            "Input sentence: Really?\n",
            "Decoded sentence: Порате                                                       \n",
            "-\n",
            "Input sentence: Come on!\n",
            "Decoded sentence: По                                                           \n",
            "-\n",
            "Input sentence: Help me!\n",
            "Decoded sentence: Он                                                           \n",
            "-\n",
            "Input sentence: I'm ill.\n",
            "Decoded sentence: Я не                                                         \n",
            "-\n",
            "Input sentence: Perfect!\n",
            "Decoded sentence: Порате                                                       \n",
            "-\n",
            "Input sentence: Who won?\n",
            "Decoded sentence: Том                                                          \n",
            "-\n",
            "Input sentence: Don't go.\n",
            "Decoded sentence: Пом                                                          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evu1qDTLbQVs"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import tensorflow as tf\n",
        "data_path = '/content/drive/My Drive/data/rus-eng/rus.txt'\n",
        "num_samples = 10000\n",
        "\n",
        "#tf.enable_eager_execution()\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = re.sub(r\"[^a-zA-Zа-яА-Я]+\", \" \", w)\n",
        "    w = w.strip()\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w\n",
        "\n",
        "\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xzj9cBWwkOwQ"
      },
      "source": [
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split('\\t')\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(preprocess_sentence(input_text))\n",
        "    target_texts.append(preprocess_sentence(target_text))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUmrOqqGbQV0",
        "outputId": "2a70ef97-509b-4aa9-c210-fda4f6fafbcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "input_texts[:2], target_texts[:2]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['<start> Go <end>', '<start> Go <end>'],\n",
              " ['<start> Марш <end>', '<start> Иди <end>'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoyE6ZsUbQWP"
      },
      "source": [
        "def tokenize(lang):\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKFdYsorbQWW"
      },
      "source": [
        "input_tensor, inp_lang_tokenizer = tokenize(input_texts)\n",
        "target_tensor, targ_lang_tokenizer = tokenize(target_texts)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXJR-aHebQWb"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9CfqRhFbQWf"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "\n",
        "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EsaBG1WbQWl"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True)\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.lstm(x, initial_state = hidden)\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "    \n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "    \n",
        "    \n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.GRU(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        output, state = self.lstm(x)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        x = self.fc(output)\n",
        "        return x, state, attention_weights"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC37jfrKbQWr"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q86risLybQWz"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "        dec_hidden = enc_hidden\n",
        "        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "    \n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    return batch_loss"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvG1BFIgbQW6",
        "outputId": "1bcd39f3-8dfa-47a0-b9e5-a5f130f069af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "EPOCHS = 15\n",
        "for epoch in range(EPOCHS):\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss 1.7894\n",
            "Epoch 2 Loss 1.4248\n",
            "Epoch 3 Loss 1.2529\n",
            "Epoch 4 Loss 1.0930\n",
            "Epoch 5 Loss 0.9318\n",
            "Epoch 6 Loss 0.7830\n",
            "Epoch 7 Loss 0.6405\n",
            "Epoch 8 Loss 0.5197\n",
            "Epoch 9 Loss 0.4257\n",
            "Epoch 10 Loss 0.3521\n",
            "Epoch 11 Loss 0.3047\n",
            "Epoch 12 Loss 0.2695\n",
            "Epoch 13 Loss 0.2421\n",
            "Epoch 14 Loss 0.2234\n",
            "Epoch 15 Loss 0.2089\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ImCME32bQXA"
      },
      "source": [
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import ticker\n",
        "\n",
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence, attention_plot\n",
        "\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "    fontdict = {'fontsize': 14}\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    plt.show()\n",
        "    \n",
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqK6cCKjbQXI",
        "outputId": "826562bb-9b32-4a65-d8d4-f75b76f53fd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "%pylab inline\n",
        "\n",
        "translate(u'good morning')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n",
            "Input: <start> good morning <end>\n",
            "Predicted translation: спокойной ночи <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['f', 'char']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfH1WCWYbQXN",
        "outputId": "db1d0a7a-7e64-4cf4-92dd-8d5a71f85d86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        }
      },
      "source": [
        "%pylab inline\n",
        "\n",
        "translate(u'good for you')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n",
            "Input: <start> good for you <end>\n",
            "Predicted translation: рад за вас <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAOjiQ6hbQXT"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "    return output, attention_weights"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4E9UarAbQXZ"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    \n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \n",
        "        \"\"\"Разделение последней размерности на (num_heads, depth).\n",
        "        Транспонирование реультата к размерности (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        \n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights\n",
        "    \n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcF8I2S3bQXe"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2\n",
        "    \n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    \n",
        "    def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n",
        "    \n",
        "    \n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                                self.d_model)\n",
        "\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                           for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mID4uwq1bQXj"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                           for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                 look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfGyM-exbQXo"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                               input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                               target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, enc_padding_mask, \n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktGQa799bQXq"
      },
      "source": [
        "# \n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "\n",
        "input_vocab_size = len(inp_lang_tokenizer.index_word) + 2\n",
        "target_vocab_size = len(targ_lang_tokenizer.index_word) + 2\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2CxlJmsbQXt"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44YPUNijbQXw"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "  \n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB_k7EFZbQXy"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jwSyXGPbQX1"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n",
        "\n",
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXlGCcu6bQX4"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR2Lo0oYbQX7"
      },
      "source": [
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "  \n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  \n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(inp, tar_inp, \n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar_real, predictions)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e8tprvFbQYA",
        "outputId": "c8a56206-4b8b-4985-96bd-2d4672a99811",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "for epoch in range(100):\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        train_step(inp, tar)\n",
        "        if batch % 50 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "    \n",
        "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss.result(), \n",
        "                                                train_accuracy.result()))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 8.3817 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 8.1891 Accuracy 0.0514\n",
            "Epoch 1 Batch 100 Loss 7.9532 Accuracy 0.0810\n",
            "Epoch 1 Loss 7.8648 Accuracy 0.0868\n",
            "Epoch 2 Batch 0 Loss 7.3744 Accuracy 0.1111\n",
            "Epoch 2 Batch 50 Loss 7.2677 Accuracy 0.1111\n",
            "Epoch 2 Batch 100 Loss 7.0952 Accuracy 0.1111\n",
            "Epoch 2 Loss 7.0061 Accuracy 0.1116\n",
            "Epoch 3 Batch 0 Loss 6.4509 Accuracy 0.1181\n",
            "Epoch 3 Batch 50 Loss 6.2517 Accuracy 0.1334\n",
            "Epoch 3 Batch 100 Loss 6.0363 Accuracy 0.1365\n",
            "Epoch 3 Loss 5.9364 Accuracy 0.1373\n",
            "Epoch 4 Batch 0 Loss 5.4796 Accuracy 0.1319\n",
            "Epoch 4 Batch 50 Loss 5.2142 Accuracy 0.1403\n",
            "Epoch 4 Batch 100 Loss 5.0787 Accuracy 0.1418\n",
            "Epoch 4 Loss 5.0204 Accuracy 0.1435\n",
            "Epoch 5 Batch 0 Loss 4.7207 Accuracy 0.1424\n",
            "Epoch 5 Batch 50 Loss 4.5678 Accuracy 0.1553\n",
            "Epoch 5 Batch 100 Loss 4.4916 Accuracy 0.1613\n",
            "Epoch 5 Loss 4.4458 Accuracy 0.1641\n",
            "Epoch 6 Batch 0 Loss 4.4326 Accuracy 0.1597\n",
            "Epoch 6 Batch 50 Loss 4.1376 Accuracy 0.1721\n",
            "Epoch 6 Batch 100 Loss 4.0666 Accuracy 0.1744\n",
            "Epoch 6 Loss 4.0411 Accuracy 0.1751\n",
            "Epoch 7 Batch 0 Loss 3.9276 Accuracy 0.1753\n",
            "Epoch 7 Batch 50 Loss 3.7971 Accuracy 0.1802\n",
            "Epoch 7 Batch 100 Loss 3.7741 Accuracy 0.1807\n",
            "Epoch 7 Loss 3.7538 Accuracy 0.1812\n",
            "Epoch 8 Batch 0 Loss 3.5349 Accuracy 0.1927\n",
            "Epoch 8 Batch 50 Loss 3.5451 Accuracy 0.1861\n",
            "Epoch 8 Batch 100 Loss 3.5200 Accuracy 0.1874\n",
            "Epoch 8 Loss 3.5128 Accuracy 0.1878\n",
            "Epoch 9 Batch 0 Loss 3.1008 Accuracy 0.2014\n",
            "Epoch 9 Batch 50 Loss 3.3203 Accuracy 0.1928\n",
            "Epoch 9 Batch 100 Loss 3.3124 Accuracy 0.1936\n",
            "Epoch 9 Loss 3.3054 Accuracy 0.1937\n",
            "Epoch 10 Batch 0 Loss 3.3069 Accuracy 0.1944\n",
            "Epoch 10 Batch 50 Loss 3.1270 Accuracy 0.1973\n",
            "Epoch 10 Batch 100 Loss 3.1255 Accuracy 0.1965\n",
            "Epoch 10 Loss 3.1112 Accuracy 0.1972\n",
            "Epoch 11 Batch 0 Loss 2.6513 Accuracy 0.2240\n",
            "Epoch 11 Batch 50 Loss 2.9308 Accuracy 0.2034\n",
            "Epoch 11 Batch 100 Loss 2.9193 Accuracy 0.2035\n",
            "Epoch 11 Loss 2.9151 Accuracy 0.2041\n",
            "Epoch 12 Batch 0 Loss 2.8341 Accuracy 0.2083\n",
            "Epoch 12 Batch 50 Loss 2.7573 Accuracy 0.2081\n",
            "Epoch 12 Batch 100 Loss 2.7422 Accuracy 0.2080\n",
            "Epoch 12 Loss 2.7400 Accuracy 0.2083\n",
            "Epoch 13 Batch 0 Loss 2.7354 Accuracy 0.2135\n",
            "Epoch 13 Batch 50 Loss 2.5493 Accuracy 0.2140\n",
            "Epoch 13 Batch 100 Loss 2.5615 Accuracy 0.2143\n",
            "Epoch 13 Loss 2.5625 Accuracy 0.2144\n",
            "Epoch 14 Batch 0 Loss 2.6605 Accuracy 0.2083\n",
            "Epoch 14 Batch 50 Loss 2.3762 Accuracy 0.2192\n",
            "Epoch 14 Batch 100 Loss 2.3818 Accuracy 0.2207\n",
            "Epoch 14 Loss 2.3858 Accuracy 0.2211\n",
            "Epoch 15 Batch 0 Loss 2.0810 Accuracy 0.2431\n",
            "Epoch 15 Batch 50 Loss 2.1869 Accuracy 0.2290\n",
            "Epoch 15 Batch 100 Loss 2.2311 Accuracy 0.2270\n",
            "Epoch 15 Loss 2.2337 Accuracy 0.2270\n",
            "Epoch 16 Batch 0 Loss 2.1236 Accuracy 0.2205\n",
            "Epoch 16 Batch 50 Loss 2.0439 Accuracy 0.2354\n",
            "Epoch 16 Batch 100 Loss 2.0649 Accuracy 0.2340\n",
            "Epoch 16 Loss 2.0686 Accuracy 0.2342\n",
            "Epoch 17 Batch 0 Loss 1.8324 Accuracy 0.2431\n",
            "Epoch 17 Batch 50 Loss 1.8730 Accuracy 0.2436\n",
            "Epoch 17 Batch 100 Loss 1.9003 Accuracy 0.2422\n",
            "Epoch 17 Loss 1.9028 Accuracy 0.2420\n",
            "Epoch 18 Batch 0 Loss 1.7099 Accuracy 0.2656\n",
            "Epoch 18 Batch 50 Loss 1.7365 Accuracy 0.2500\n",
            "Epoch 18 Batch 100 Loss 1.7554 Accuracy 0.2468\n",
            "Epoch 18 Loss 1.7633 Accuracy 0.2472\n",
            "Epoch 19 Batch 0 Loss 1.5946 Accuracy 0.2639\n",
            "Epoch 19 Batch 50 Loss 1.5825 Accuracy 0.2581\n",
            "Epoch 19 Batch 100 Loss 1.6069 Accuracy 0.2549\n",
            "Epoch 19 Loss 1.6178 Accuracy 0.2542\n",
            "Epoch 20 Batch 0 Loss 1.3783 Accuracy 0.2778\n",
            "Epoch 20 Batch 50 Loss 1.4485 Accuracy 0.2630\n",
            "Epoch 20 Batch 100 Loss 1.4808 Accuracy 0.2612\n",
            "Epoch 20 Loss 1.4887 Accuracy 0.2607\n",
            "Epoch 21 Batch 0 Loss 1.4060 Accuracy 0.2552\n",
            "Epoch 21 Batch 50 Loss 1.3135 Accuracy 0.2712\n",
            "Epoch 21 Batch 100 Loss 1.3454 Accuracy 0.2683\n",
            "Epoch 21 Loss 1.3691 Accuracy 0.2660\n",
            "Epoch 22 Batch 0 Loss 1.1910 Accuracy 0.2726\n",
            "Epoch 22 Batch 50 Loss 1.1700 Accuracy 0.2775\n",
            "Epoch 22 Batch 100 Loss 1.2341 Accuracy 0.2728\n",
            "Epoch 22 Loss 1.2572 Accuracy 0.2708\n",
            "Epoch 23 Batch 0 Loss 1.0591 Accuracy 0.2743\n",
            "Epoch 23 Batch 50 Loss 1.0882 Accuracy 0.2843\n",
            "Epoch 23 Batch 100 Loss 1.1524 Accuracy 0.2777\n",
            "Epoch 23 Loss 1.1643 Accuracy 0.2761\n",
            "Epoch 24 Batch 0 Loss 0.9635 Accuracy 0.3021\n",
            "Epoch 24 Batch 50 Loss 1.0175 Accuracy 0.2833\n",
            "Epoch 24 Batch 100 Loss 1.0701 Accuracy 0.2796\n",
            "Epoch 24 Loss 1.0933 Accuracy 0.2784\n",
            "Epoch 25 Batch 0 Loss 0.8867 Accuracy 0.3090\n",
            "Epoch 25 Batch 50 Loss 0.9222 Accuracy 0.2939\n",
            "Epoch 25 Batch 100 Loss 0.9861 Accuracy 0.2862\n",
            "Epoch 25 Loss 1.0141 Accuracy 0.2839\n",
            "Epoch 26 Batch 0 Loss 0.8496 Accuracy 0.3212\n",
            "Epoch 26 Batch 50 Loss 0.9006 Accuracy 0.2932\n",
            "Epoch 26 Batch 100 Loss 0.9543 Accuracy 0.2877\n",
            "Epoch 26 Loss 0.9788 Accuracy 0.2860\n",
            "Epoch 27 Batch 0 Loss 0.8138 Accuracy 0.2795\n",
            "Epoch 27 Batch 50 Loss 0.8285 Accuracy 0.2978\n",
            "Epoch 27 Batch 100 Loss 0.8939 Accuracy 0.2914\n",
            "Epoch 27 Loss 0.9206 Accuracy 0.2886\n",
            "Epoch 28 Batch 0 Loss 0.8300 Accuracy 0.3003\n",
            "Epoch 28 Batch 50 Loss 0.7955 Accuracy 0.3006\n",
            "Epoch 28 Batch 100 Loss 0.8628 Accuracy 0.2943\n",
            "Epoch 28 Loss 0.8911 Accuracy 0.2916\n",
            "Epoch 29 Batch 0 Loss 0.6348 Accuracy 0.3333\n",
            "Epoch 29 Batch 50 Loss 0.7585 Accuracy 0.3041\n",
            "Epoch 29 Batch 100 Loss 0.8332 Accuracy 0.2962\n",
            "Epoch 29 Loss 0.8658 Accuracy 0.2932\n",
            "Epoch 30 Batch 0 Loss 0.7102 Accuracy 0.3073\n",
            "Epoch 30 Batch 50 Loss 0.7465 Accuracy 0.3053\n",
            "Epoch 30 Batch 100 Loss 0.7974 Accuracy 0.2992\n",
            "Epoch 30 Loss 0.8282 Accuracy 0.2957\n",
            "Epoch 31 Batch 0 Loss 0.5969 Accuracy 0.3108\n",
            "Epoch 31 Batch 50 Loss 0.7495 Accuracy 0.3029\n",
            "Epoch 31 Batch 100 Loss 0.8120 Accuracy 0.2971\n",
            "Epoch 31 Loss 0.8423 Accuracy 0.2945\n",
            "Epoch 32 Batch 0 Loss 0.5879 Accuracy 0.3125\n",
            "Epoch 32 Batch 50 Loss 0.7237 Accuracy 0.3053\n",
            "Epoch 32 Batch 100 Loss 0.7901 Accuracy 0.2998\n",
            "Epoch 32 Loss 0.8299 Accuracy 0.2959\n",
            "Epoch 33 Batch 0 Loss 0.5886 Accuracy 0.3108\n",
            "Epoch 33 Batch 50 Loss 0.6979 Accuracy 0.3077\n",
            "Epoch 33 Batch 100 Loss 0.7758 Accuracy 0.3001\n",
            "Epoch 33 Loss 0.8071 Accuracy 0.2966\n",
            "Epoch 34 Batch 0 Loss 0.7733 Accuracy 0.2795\n",
            "Epoch 34 Batch 50 Loss 0.7086 Accuracy 0.3050\n",
            "Epoch 34 Batch 100 Loss 0.7699 Accuracy 0.3006\n",
            "Epoch 34 Loss 0.7956 Accuracy 0.2977\n",
            "Epoch 35 Batch 0 Loss 0.5317 Accuracy 0.3351\n",
            "Epoch 35 Batch 50 Loss 0.6613 Accuracy 0.3093\n",
            "Epoch 35 Batch 100 Loss 0.7289 Accuracy 0.3030\n",
            "Epoch 35 Loss 0.7524 Accuracy 0.3014\n",
            "Epoch 36 Batch 0 Loss 0.5863 Accuracy 0.3073\n",
            "Epoch 36 Batch 50 Loss 0.6372 Accuracy 0.3126\n",
            "Epoch 36 Batch 100 Loss 0.7012 Accuracy 0.3063\n",
            "Epoch 36 Loss 0.7329 Accuracy 0.3036\n",
            "Epoch 37 Batch 0 Loss 0.6076 Accuracy 0.2899\n",
            "Epoch 37 Batch 50 Loss 0.6433 Accuracy 0.3122\n",
            "Epoch 37 Batch 100 Loss 0.7016 Accuracy 0.3069\n",
            "Epoch 37 Loss 0.7274 Accuracy 0.3047\n",
            "Epoch 38 Batch 0 Loss 0.5141 Accuracy 0.3333\n",
            "Epoch 38 Batch 50 Loss 0.6061 Accuracy 0.3158\n",
            "Epoch 38 Batch 100 Loss 0.6688 Accuracy 0.3094\n",
            "Epoch 38 Loss 0.6973 Accuracy 0.3070\n",
            "Epoch 39 Batch 0 Loss 0.4672 Accuracy 0.3385\n",
            "Epoch 39 Batch 50 Loss 0.6018 Accuracy 0.3152\n",
            "Epoch 39 Batch 100 Loss 0.6659 Accuracy 0.3092\n",
            "Epoch 39 Loss 0.6893 Accuracy 0.3069\n",
            "Epoch 40 Batch 0 Loss 0.4327 Accuracy 0.3420\n",
            "Epoch 40 Batch 50 Loss 0.5878 Accuracy 0.3188\n",
            "Epoch 40 Batch 100 Loss 0.6559 Accuracy 0.3103\n",
            "Epoch 40 Loss 0.6742 Accuracy 0.3082\n",
            "Epoch 41 Batch 0 Loss 0.5812 Accuracy 0.3194\n",
            "Epoch 41 Batch 50 Loss 0.5940 Accuracy 0.3159\n",
            "Epoch 41 Batch 100 Loss 0.6491 Accuracy 0.3102\n",
            "Epoch 41 Loss 0.6723 Accuracy 0.3079\n",
            "Epoch 42 Batch 0 Loss 0.5447 Accuracy 0.3177\n",
            "Epoch 42 Batch 50 Loss 0.5609 Accuracy 0.3155\n",
            "Epoch 42 Batch 100 Loss 0.6202 Accuracy 0.3127\n",
            "Epoch 42 Loss 0.6467 Accuracy 0.3110\n",
            "Epoch 43 Batch 0 Loss 0.4464 Accuracy 0.3160\n",
            "Epoch 43 Batch 50 Loss 0.5528 Accuracy 0.3196\n",
            "Epoch 43 Batch 100 Loss 0.6101 Accuracy 0.3143\n",
            "Epoch 43 Loss 0.6356 Accuracy 0.3114\n",
            "Epoch 44 Batch 0 Loss 0.4793 Accuracy 0.3333\n",
            "Epoch 44 Batch 50 Loss 0.5568 Accuracy 0.3172\n",
            "Epoch 44 Batch 100 Loss 0.6093 Accuracy 0.3137\n",
            "Epoch 44 Loss 0.6338 Accuracy 0.3109\n",
            "Epoch 45 Batch 0 Loss 0.4751 Accuracy 0.3194\n",
            "Epoch 45 Batch 50 Loss 0.5494 Accuracy 0.3189\n",
            "Epoch 45 Batch 100 Loss 0.6013 Accuracy 0.3147\n",
            "Epoch 45 Loss 0.6232 Accuracy 0.3127\n",
            "Epoch 46 Batch 0 Loss 0.4349 Accuracy 0.3368\n",
            "Epoch 46 Batch 50 Loss 0.5291 Accuracy 0.3237\n",
            "Epoch 46 Batch 100 Loss 0.5826 Accuracy 0.3168\n",
            "Epoch 46 Loss 0.6139 Accuracy 0.3133\n",
            "Epoch 47 Batch 0 Loss 0.4552 Accuracy 0.3281\n",
            "Epoch 47 Batch 50 Loss 0.5236 Accuracy 0.3208\n",
            "Epoch 47 Batch 100 Loss 0.5924 Accuracy 0.3146\n",
            "Epoch 47 Loss 0.6166 Accuracy 0.3123\n",
            "Epoch 48 Batch 0 Loss 0.3914 Accuracy 0.3333\n",
            "Epoch 48 Batch 50 Loss 0.5133 Accuracy 0.3233\n",
            "Epoch 48 Batch 100 Loss 0.5744 Accuracy 0.3171\n",
            "Epoch 48 Loss 0.5955 Accuracy 0.3154\n",
            "Epoch 49 Batch 0 Loss 0.4266 Accuracy 0.3368\n",
            "Epoch 49 Batch 50 Loss 0.5049 Accuracy 0.3240\n",
            "Epoch 49 Batch 100 Loss 0.5646 Accuracy 0.3171\n",
            "Epoch 49 Loss 0.5895 Accuracy 0.3142\n",
            "Epoch 50 Batch 0 Loss 0.4558 Accuracy 0.3385\n",
            "Epoch 50 Batch 50 Loss 0.5042 Accuracy 0.3246\n",
            "Epoch 50 Batch 100 Loss 0.5581 Accuracy 0.3185\n",
            "Epoch 50 Loss 0.5850 Accuracy 0.3156\n",
            "Epoch 51 Batch 0 Loss 0.4163 Accuracy 0.3507\n",
            "Epoch 51 Batch 50 Loss 0.4981 Accuracy 0.3226\n",
            "Epoch 51 Batch 100 Loss 0.5603 Accuracy 0.3176\n",
            "Epoch 51 Loss 0.5775 Accuracy 0.3159\n",
            "Epoch 52 Batch 0 Loss 0.4738 Accuracy 0.3212\n",
            "Epoch 52 Batch 50 Loss 0.5034 Accuracy 0.3221\n",
            "Epoch 52 Batch 100 Loss 0.5565 Accuracy 0.3173\n",
            "Epoch 52 Loss 0.5690 Accuracy 0.3162\n",
            "Epoch 53 Batch 0 Loss 0.4007 Accuracy 0.3160\n",
            "Epoch 53 Batch 50 Loss 0.4976 Accuracy 0.3229\n",
            "Epoch 53 Batch 100 Loss 0.5462 Accuracy 0.3172\n",
            "Epoch 53 Loss 0.5627 Accuracy 0.3160\n",
            "Epoch 54 Batch 0 Loss 0.4617 Accuracy 0.3316\n",
            "Epoch 54 Batch 50 Loss 0.4869 Accuracy 0.3234\n",
            "Epoch 54 Batch 100 Loss 0.5407 Accuracy 0.3194\n",
            "Epoch 54 Loss 0.5600 Accuracy 0.3172\n",
            "Epoch 55 Batch 0 Loss 0.4198 Accuracy 0.3229\n",
            "Epoch 55 Batch 50 Loss 0.4731 Accuracy 0.3254\n",
            "Epoch 55 Batch 100 Loss 0.5266 Accuracy 0.3208\n",
            "Epoch 55 Loss 0.5504 Accuracy 0.3179\n",
            "Epoch 56 Batch 0 Loss 0.4854 Accuracy 0.2986\n",
            "Epoch 56 Batch 50 Loss 0.4709 Accuracy 0.3249\n",
            "Epoch 56 Batch 100 Loss 0.5255 Accuracy 0.3212\n",
            "Epoch 56 Loss 0.5498 Accuracy 0.3179\n",
            "Epoch 57 Batch 0 Loss 0.3265 Accuracy 0.3507\n",
            "Epoch 57 Batch 50 Loss 0.4733 Accuracy 0.3268\n",
            "Epoch 57 Batch 100 Loss 0.5232 Accuracy 0.3213\n",
            "Epoch 57 Loss 0.5467 Accuracy 0.3179\n",
            "Epoch 58 Batch 0 Loss 0.4272 Accuracy 0.3333\n",
            "Epoch 58 Batch 50 Loss 0.4668 Accuracy 0.3271\n",
            "Epoch 58 Batch 100 Loss 0.5177 Accuracy 0.3201\n",
            "Epoch 58 Loss 0.5376 Accuracy 0.3186\n",
            "Epoch 59 Batch 0 Loss 0.4194 Accuracy 0.3229\n",
            "Epoch 59 Batch 50 Loss 0.4655 Accuracy 0.3251\n",
            "Epoch 59 Batch 100 Loss 0.5111 Accuracy 0.3217\n",
            "Epoch 59 Loss 0.5340 Accuracy 0.3199\n",
            "Epoch 60 Batch 0 Loss 0.3951 Accuracy 0.3385\n",
            "Epoch 60 Batch 50 Loss 0.4553 Accuracy 0.3271\n",
            "Epoch 60 Batch 100 Loss 0.5152 Accuracy 0.3208\n",
            "Epoch 60 Loss 0.5332 Accuracy 0.3194\n",
            "Epoch 61 Batch 0 Loss 0.4278 Accuracy 0.3420\n",
            "Epoch 61 Batch 50 Loss 0.4570 Accuracy 0.3281\n",
            "Epoch 61 Batch 100 Loss 0.5085 Accuracy 0.3212\n",
            "Epoch 61 Loss 0.5220 Accuracy 0.3200\n",
            "Epoch 62 Batch 0 Loss 0.3538 Accuracy 0.3351\n",
            "Epoch 62 Batch 50 Loss 0.4599 Accuracy 0.3252\n",
            "Epoch 62 Batch 100 Loss 0.5047 Accuracy 0.3212\n",
            "Epoch 62 Loss 0.5265 Accuracy 0.3194\n",
            "Epoch 63 Batch 0 Loss 0.4175 Accuracy 0.3507\n",
            "Epoch 63 Batch 50 Loss 0.4566 Accuracy 0.3261\n",
            "Epoch 63 Batch 100 Loss 0.4937 Accuracy 0.3234\n",
            "Epoch 63 Loss 0.5150 Accuracy 0.3210\n",
            "Epoch 64 Batch 0 Loss 0.3592 Accuracy 0.3351\n",
            "Epoch 64 Batch 50 Loss 0.4451 Accuracy 0.3266\n",
            "Epoch 64 Batch 100 Loss 0.4965 Accuracy 0.3223\n",
            "Epoch 64 Loss 0.5142 Accuracy 0.3206\n",
            "Epoch 65 Batch 0 Loss 0.3400 Accuracy 0.3507\n",
            "Epoch 65 Batch 50 Loss 0.4430 Accuracy 0.3272\n",
            "Epoch 65 Batch 100 Loss 0.4931 Accuracy 0.3218\n",
            "Epoch 65 Loss 0.5093 Accuracy 0.3208\n",
            "Epoch 66 Batch 0 Loss 0.3978 Accuracy 0.3316\n",
            "Epoch 66 Batch 50 Loss 0.4451 Accuracy 0.3285\n",
            "Epoch 66 Batch 100 Loss 0.4935 Accuracy 0.3221\n",
            "Epoch 66 Loss 0.5131 Accuracy 0.3204\n",
            "Epoch 67 Batch 0 Loss 0.3440 Accuracy 0.3646\n",
            "Epoch 67 Batch 50 Loss 0.4460 Accuracy 0.3257\n",
            "Epoch 67 Batch 100 Loss 0.4863 Accuracy 0.3228\n",
            "Epoch 67 Loss 0.5057 Accuracy 0.3212\n",
            "Epoch 68 Batch 0 Loss 0.3749 Accuracy 0.3403\n",
            "Epoch 68 Batch 50 Loss 0.4331 Accuracy 0.3297\n",
            "Epoch 68 Batch 100 Loss 0.4786 Accuracy 0.3238\n",
            "Epoch 68 Loss 0.5004 Accuracy 0.3219\n",
            "Epoch 69 Batch 0 Loss 0.4715 Accuracy 0.3108\n",
            "Epoch 69 Batch 50 Loss 0.4220 Accuracy 0.3320\n",
            "Epoch 69 Batch 100 Loss 0.4763 Accuracy 0.3235\n",
            "Epoch 69 Loss 0.4966 Accuracy 0.3216\n",
            "Epoch 70 Batch 0 Loss 0.4110 Accuracy 0.3247\n",
            "Epoch 70 Batch 50 Loss 0.4307 Accuracy 0.3294\n",
            "Epoch 70 Batch 100 Loss 0.4722 Accuracy 0.3250\n",
            "Epoch 70 Loss 0.4931 Accuracy 0.3224\n",
            "Epoch 71 Batch 0 Loss 0.3244 Accuracy 0.3524\n",
            "Epoch 71 Batch 50 Loss 0.4315 Accuracy 0.3279\n",
            "Epoch 71 Batch 100 Loss 0.4736 Accuracy 0.3233\n",
            "Epoch 71 Loss 0.4898 Accuracy 0.3215\n",
            "Epoch 72 Batch 0 Loss 0.3405 Accuracy 0.3299\n",
            "Epoch 72 Batch 50 Loss 0.4163 Accuracy 0.3313\n",
            "Epoch 72 Batch 100 Loss 0.4699 Accuracy 0.3238\n",
            "Epoch 72 Loss 0.4849 Accuracy 0.3234\n",
            "Epoch 73 Batch 0 Loss 0.4111 Accuracy 0.3160\n",
            "Epoch 73 Batch 50 Loss 0.4262 Accuracy 0.3292\n",
            "Epoch 73 Batch 100 Loss 0.4676 Accuracy 0.3238\n",
            "Epoch 73 Loss 0.4872 Accuracy 0.3218\n",
            "Epoch 74 Batch 0 Loss 0.5032 Accuracy 0.3177\n",
            "Epoch 74 Batch 50 Loss 0.4282 Accuracy 0.3257\n",
            "Epoch 74 Batch 100 Loss 0.4637 Accuracy 0.3226\n",
            "Epoch 74 Loss 0.4875 Accuracy 0.3207\n",
            "Epoch 75 Batch 0 Loss 0.4353 Accuracy 0.3212\n",
            "Epoch 75 Batch 50 Loss 0.4124 Accuracy 0.3292\n",
            "Epoch 75 Batch 100 Loss 0.4618 Accuracy 0.3245\n",
            "Epoch 75 Loss 0.4820 Accuracy 0.3225\n",
            "Epoch 76 Batch 0 Loss 0.4129 Accuracy 0.3229\n",
            "Epoch 76 Batch 50 Loss 0.4100 Accuracy 0.3312\n",
            "Epoch 76 Batch 100 Loss 0.4564 Accuracy 0.3266\n",
            "Epoch 76 Loss 0.4748 Accuracy 0.3236\n",
            "Epoch 77 Batch 0 Loss 0.3830 Accuracy 0.3316\n",
            "Epoch 77 Batch 50 Loss 0.4198 Accuracy 0.3280\n",
            "Epoch 77 Batch 100 Loss 0.4571 Accuracy 0.3238\n",
            "Epoch 77 Loss 0.4731 Accuracy 0.3228\n",
            "Epoch 78 Batch 0 Loss 0.3444 Accuracy 0.3420\n",
            "Epoch 78 Batch 50 Loss 0.4186 Accuracy 0.3267\n",
            "Epoch 78 Batch 100 Loss 0.4543 Accuracy 0.3254\n",
            "Epoch 78 Loss 0.4710 Accuracy 0.3234\n",
            "Epoch 79 Batch 0 Loss 0.2642 Accuracy 0.3594\n",
            "Epoch 79 Batch 50 Loss 0.4143 Accuracy 0.3303\n",
            "Epoch 79 Batch 100 Loss 0.4512 Accuracy 0.3253\n",
            "Epoch 79 Loss 0.4702 Accuracy 0.3236\n",
            "Epoch 80 Batch 0 Loss 0.3943 Accuracy 0.3316\n",
            "Epoch 80 Batch 50 Loss 0.4035 Accuracy 0.3289\n",
            "Epoch 80 Batch 100 Loss 0.4551 Accuracy 0.3245\n",
            "Epoch 80 Loss 0.4709 Accuracy 0.3227\n",
            "Epoch 81 Batch 0 Loss 0.3612 Accuracy 0.3229\n",
            "Epoch 81 Batch 50 Loss 0.4071 Accuracy 0.3307\n",
            "Epoch 81 Batch 100 Loss 0.4455 Accuracy 0.3258\n",
            "Epoch 81 Loss 0.4639 Accuracy 0.3235\n",
            "Epoch 82 Batch 0 Loss 0.3907 Accuracy 0.3333\n",
            "Epoch 82 Batch 50 Loss 0.4082 Accuracy 0.3297\n",
            "Epoch 82 Batch 100 Loss 0.4475 Accuracy 0.3254\n",
            "Epoch 82 Loss 0.4661 Accuracy 0.3240\n",
            "Epoch 83 Batch 0 Loss 0.3638 Accuracy 0.3333\n",
            "Epoch 83 Batch 50 Loss 0.4142 Accuracy 0.3279\n",
            "Epoch 83 Batch 100 Loss 0.4515 Accuracy 0.3248\n",
            "Epoch 83 Loss 0.4673 Accuracy 0.3232\n",
            "Epoch 84 Batch 0 Loss 0.3072 Accuracy 0.3490\n",
            "Epoch 84 Batch 50 Loss 0.4076 Accuracy 0.3256\n",
            "Epoch 84 Batch 100 Loss 0.4439 Accuracy 0.3243\n",
            "Epoch 84 Loss 0.4634 Accuracy 0.3230\n",
            "Epoch 85 Batch 0 Loss 0.3528 Accuracy 0.3507\n",
            "Epoch 85 Batch 50 Loss 0.4009 Accuracy 0.3302\n",
            "Epoch 85 Batch 100 Loss 0.4383 Accuracy 0.3254\n",
            "Epoch 85 Loss 0.4569 Accuracy 0.3238\n",
            "Epoch 86 Batch 0 Loss 0.4452 Accuracy 0.3073\n",
            "Epoch 86 Batch 50 Loss 0.3947 Accuracy 0.3327\n",
            "Epoch 86 Batch 100 Loss 0.4376 Accuracy 0.3258\n",
            "Epoch 86 Loss 0.4579 Accuracy 0.3239\n",
            "Epoch 87 Batch 0 Loss 0.3089 Accuracy 0.3333\n",
            "Epoch 87 Batch 50 Loss 0.3958 Accuracy 0.3290\n",
            "Epoch 87 Batch 100 Loss 0.4380 Accuracy 0.3259\n",
            "Epoch 87 Loss 0.4561 Accuracy 0.3240\n",
            "Epoch 88 Batch 0 Loss 0.4035 Accuracy 0.3090\n",
            "Epoch 88 Batch 50 Loss 0.3831 Accuracy 0.3310\n",
            "Epoch 88 Batch 100 Loss 0.4262 Accuracy 0.3262\n",
            "Epoch 88 Loss 0.4491 Accuracy 0.3241\n",
            "Epoch 89 Batch 0 Loss 0.3475 Accuracy 0.3351\n",
            "Epoch 89 Batch 50 Loss 0.3949 Accuracy 0.3276\n",
            "Epoch 89 Batch 100 Loss 0.4311 Accuracy 0.3256\n",
            "Epoch 89 Loss 0.4493 Accuracy 0.3239\n",
            "Epoch 90 Batch 0 Loss 0.3613 Accuracy 0.3281\n",
            "Epoch 90 Batch 50 Loss 0.3927 Accuracy 0.3272\n",
            "Epoch 90 Batch 100 Loss 0.4326 Accuracy 0.3255\n",
            "Epoch 90 Loss 0.4510 Accuracy 0.3235\n",
            "Epoch 91 Batch 0 Loss 0.3917 Accuracy 0.3177\n",
            "Epoch 91 Batch 50 Loss 0.3832 Accuracy 0.3307\n",
            "Epoch 91 Batch 100 Loss 0.4279 Accuracy 0.3267\n",
            "Epoch 91 Loss 0.4451 Accuracy 0.3247\n",
            "Epoch 92 Batch 0 Loss 0.3505 Accuracy 0.3385\n",
            "Epoch 92 Batch 50 Loss 0.3899 Accuracy 0.3295\n",
            "Epoch 92 Batch 100 Loss 0.4284 Accuracy 0.3256\n",
            "Epoch 92 Loss 0.4451 Accuracy 0.3242\n",
            "Epoch 93 Batch 0 Loss 0.3245 Accuracy 0.3247\n",
            "Epoch 93 Batch 50 Loss 0.3830 Accuracy 0.3320\n",
            "Epoch 93 Batch 100 Loss 0.4268 Accuracy 0.3262\n",
            "Epoch 93 Loss 0.4401 Accuracy 0.3246\n",
            "Epoch 94 Batch 0 Loss 0.3683 Accuracy 0.3472\n",
            "Epoch 94 Batch 50 Loss 0.3962 Accuracy 0.3289\n",
            "Epoch 94 Batch 100 Loss 0.4257 Accuracy 0.3255\n",
            "Epoch 94 Loss 0.4453 Accuracy 0.3237\n",
            "Epoch 95 Batch 0 Loss 0.4297 Accuracy 0.3385\n",
            "Epoch 95 Batch 50 Loss 0.3831 Accuracy 0.3310\n",
            "Epoch 95 Batch 100 Loss 0.4243 Accuracy 0.3264\n",
            "Epoch 95 Loss 0.4378 Accuracy 0.3252\n",
            "Epoch 96 Batch 0 Loss 0.3675 Accuracy 0.3438\n",
            "Epoch 96 Batch 50 Loss 0.3764 Accuracy 0.3305\n",
            "Epoch 96 Batch 100 Loss 0.4231 Accuracy 0.3261\n",
            "Epoch 96 Loss 0.4389 Accuracy 0.3248\n",
            "Epoch 97 Batch 0 Loss 0.3893 Accuracy 0.3385\n",
            "Epoch 97 Batch 50 Loss 0.3747 Accuracy 0.3308\n",
            "Epoch 97 Batch 100 Loss 0.4204 Accuracy 0.3268\n",
            "Epoch 97 Loss 0.4365 Accuracy 0.3249\n",
            "Epoch 98 Batch 0 Loss 0.3639 Accuracy 0.3472\n",
            "Epoch 98 Batch 50 Loss 0.3729 Accuracy 0.3326\n",
            "Epoch 98 Batch 100 Loss 0.4197 Accuracy 0.3263\n",
            "Epoch 98 Loss 0.4375 Accuracy 0.3247\n",
            "Epoch 99 Batch 0 Loss 0.3471 Accuracy 0.3333\n",
            "Epoch 99 Batch 50 Loss 0.3737 Accuracy 0.3310\n",
            "Epoch 99 Batch 100 Loss 0.4191 Accuracy 0.3256\n",
            "Epoch 99 Loss 0.4341 Accuracy 0.3249\n",
            "Epoch 100 Batch 0 Loss 0.2972 Accuracy 0.3403\n",
            "Epoch 100 Batch 50 Loss 0.3743 Accuracy 0.3323\n",
            "Epoch 100 Batch 100 Loss 0.4133 Accuracy 0.3281\n",
            "Epoch 100 Loss 0.4299 Accuracy 0.3256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQhuPAgNbQYD"
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    start_token = [1]\n",
        "    end_token = [2]\n",
        "  \n",
        "    sentence = preprocess_sentence(inp_sentence)\n",
        "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    \n",
        "    encoder_input = tf.expand_dims(inputs, 0)\n",
        "  \n",
        "  # as the target is english, the first word to the transformer should be the\n",
        "  # english start token.\n",
        "    decoder_input = [1]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(max_length_targ):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output)\n",
        "  \n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(encoder_input, \n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "    \n",
        "    # select the last word from the seq_len dimension\n",
        "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    \n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == targ_lang_tokenizer.word_index[\"<end>\"]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "    \n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "\n",
        "def plot_attention_weights(attention, sentence, result, layer):\n",
        "    fig = plt.figure(figsize=(16, 8))\n",
        "  \n",
        "    sentence = inp_lang_tokenizer.encode(sentence)\n",
        "  \n",
        "    attention = tf.squeeze(attention[layer], axis=0)\n",
        "  \n",
        "    for head in range(attention.shape[0]):\n",
        "        ax = fig.add_subplot(2, 4, head+1)\n",
        "\n",
        "        # plot the attention weights\n",
        "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
        "\n",
        "        fontdict = {'fontsize': 10}\n",
        "\n",
        "        ax.set_xticks(range(len(sentence)+2))\n",
        "        ax.set_yticks(range(len(result)))\n",
        "\n",
        "        ax.set_ylim(len(result)-1.5, -0.5)\n",
        "\n",
        "        ax.set_xticklabels(\n",
        "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
        "            fontdict=fontdict, rotation=90)\n",
        "\n",
        "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
        "                            if i < tokenizer_en.vocab_size], \n",
        "                           fontdict=fontdict)\n",
        "\n",
        "        ax.set_xlabel('Head {}'.format(head+1))\n",
        "  \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def translate(sentence, plot=''):\n",
        "    result, attention_weights = evaluate(sentence)\n",
        "    predicted_sentence = ([targ_lang_tokenizer.index_word[i] for i in result.numpy()])  \n",
        "\n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Predicted translation: {}'.format(predicted_sentence))\n",
        "  \n",
        "    if plot:\n",
        "        plot_attention_weights(attention_weights, sentence, result, plot)\n",
        "        \n",
        "# translate(\"good morning.\")"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y55gRPfjbQYG",
        "outputId": "a7d6abc1-e158-4753-ae99-87b881511ef0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "translate(\"good morning.\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: good morning.\n",
            "Predicted translation: ['<start>', 'с', 'добрым', 'утром']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azzoh9HvbQYJ",
        "outputId": "0af91823-c3c8-436b-eb01-0004d77a44f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "translate('how are you, my friend?')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: how are you, my friend?\n",
            "Predicted translation: ['<start>', 'как', 'ты', 'ровесник']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80Vz-r_ObQYN",
        "outputId": "8565622a-f08b-4718-e881-74ad55b2bdb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "translate('wake up and go work')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: wake up and go work\n",
            "Predicted translation: ['<start>', 'найди', 'работу']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BuXSZYLbQYT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}